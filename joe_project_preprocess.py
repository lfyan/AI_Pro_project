# -*- coding: utf-8 -*-
"""Joe_Project_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IgjY6Pa0XiKumPMFp2ex_iGEC3SAajJ0

###**setup environment and download dataset**
"""
#
# !pip install kaggle --upgrade
# !echo '{"username":"zhouqiangwang","key":"64845c52135420da924e4ac14ba94375"}' > /root/.kaggle/kaggle.json
# !chmod 600 /root/.kaggle/kaggle.json
# !kaggle competitions download -c quora-insincere-questions-classification
# !ls -al
#
# !unzip embeddings.zip
# !unzip sample_submission.csv.zip
# !unzip train.csv.zip
# !unzip test.csv.zip
# !ls -al
#
# !ls -al
# !pwd
# !head test.csv

"""## Toward Handling Toxic and Divisive Content on Social Media"""

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from collections import Counter
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from nltk import word_tokenize
from nltk.corpus import stopwords
import string
from PIL import Image
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

"""## Pre-processing input texts"""

df=pd.read_csv('train.csv')

df.head()

df.info()

# About 6% od the data has label 1

Counter(df['target'])

import nltk
nltk.download('stopwords')

stops=stopwords.words('english')+list(string.punctuation)

# !head glove.840B.300d/glove.840B.300d.txt

# load Glove vector
print('Loading word vectors...')
word2vec = {}
embedding = []
idx2word = []
# with open('glove.6B/glove.6B.100d.txt', encoding='utf-8') as f:
with open('glove.840B.300d/glove.840B.300d.txt', encoding='utf-8') as f:
  # is just a space-separated text file in the format:
  # word vec[0] vec[1] vec[2] ...
  ln = 0
  for line in f:
      ln += 1
      values = line.split()
      if len(values) != 301:
        print("length of %d line is %d" %(ln, len(values)))
        continue
      word = values[0]
      vec = np.asarray(values[1:], dtype='float32')
      word2vec[word] = vec
      embedding.append(vec)
      idx2word.append(word)
print('Found %s word vectors.' % len(word2vec))
embedding = np.array(embedding)
V, D = embedding.shape
word2idx={word:index_no for index_no , word in enumerate(idx2word)}

nltk.download('punkt')
print("begin to loading the biggest part......")
df['clean_text']=df['question_text'].apply(lambda x:[t for t in word_tokenize(x.lower()) if (t not in stops) & (t in idx2word)] )

print(df.head())

max(df.clean_text.apply(len))

df.shape[0]

df['clean_text_idx']=df['clean_text'].apply(lambda x:[word2idx[t] for t in x] )

df.head()

data_pad=pad_sequences(df['clean_text_idx'],maxlen=78)

data_pad[0]

labels=df['target']

labels=to_categorical(labels,num_classes=2)

"""## Wordcloud exploration for fun"""

df1=df[df['target']==1]['question_text']

text=df1.str.cat(sep=' ')

# Start with one review:
text=df1.str.cat(sep=' ')

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df0=df[df['target']==0]['question_text']

text0=df0.str.cat(sep=' ')

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text0)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""## Model Creation"""

lstm_hidden_units=64  # no of hidden units in 1 lstm cell
num_classes=2
embedding_size=100  # no of unrolling
num_epochs=1
sentence_len=78  # max sentence length in dataset
vocab_size=400000  # Glove vocab size

def model():
    
    x=tf.placeholder(tf.int32,shape=[None,sentence_len])  # input place holder
    y=tf.placeholder(tf.float32,shape=[None,2])
    
    w=tf.Variable(tf.random_normal([lstm_hidden_units,num_classes]))
    b=tf.Variable(tf.constant(0.1,shape=[num_classes]))
        
    Embedding = tf.get_variable(name="word_embedding", shape=[embedding.shape[0],embedding_size],initializer=tf.constant_initializer(embedding),trainable=False)                                
    embed_lookup=tf.nn.embedding_lookup(Embedding,x)
    
    lstm_cell=tf.contrib.rnn.BasicLSTMCell(lstm_hidden_units)
    current_batch_size=tf.shape(x)[0]
    initial_state=lstm_cell.zero_state(current_batch_size,dtype=tf.float32)
    outputs, _ =tf.nn.dynamic_rnn(lstm_cell,embed_lookup,initial_state=initial_state,dtype=tf.float32)
    outputs=tf.transpose(outputs,[1,0,2]) #output manipulation from batch_size x 78 x 64 to 78 x batch_size x 64
    last=tf.gather(outputs,int(outputs.get_shape()[0])-1)  # access the last element in the output vector
    
    predictions=tf.matmul(last,w)+b
    correct_predictions=tf.equal(tf.argmax(tf.nn.sigmoid(predictions),axis=1),tf.argmax(y,axis=1))
    accuracy=tf.reduce_mean(tf.cast(correct_predictions,tf.float32))   
    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions,labels=y))
    optimizer=tf.train.AdamOptimizer(learning_rate=0.003).minimize(loss)
    
    return optimizer,loss,x,y,accuracy,predictions, correct_predictions



"""## Training and Testing"""

X_train,X_test, y_train,y_test=train_test_split(data_pad,labels,test_size=0.3,random_state=101)

X_train.shape

X_test.shape

tf.reset_default_graph()
optimizer,loss,x,y,accuracy,predictions, correct_predictions=model()

batch_size=32
num_batches=len(X_train)//batch_size

g = tf.Graph()
with tf.Session() as sesh:
    init=tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())
    sesh.run(init)    
    saver = tf.train.Saver()
    writer= tf.summary.FileWriter("logdir/", graph = sesh.graph)    
    
    for i in range(num_batches):
        if i !=num_batches-1:
            x_batch=X_train[i*batch_size:i*batch_size+batch_size]
            y_batch=y_train[i*batch_size:i*batch_size+batch_size]
        else:
            x_batch=X_train[i*batch_size:]
            y_batch=y_train[i*batch_size:]
        
        # Randomly setting testing data to see performance when training
        rand_idx = np.random.choice(np.arange(len(X_test)),200, replace=False)
        test_x = X_test[rand_idx]
        test_y = y_test[rand_idx]
        
        _, l, a=sesh.run([optimizer,loss,accuracy],feed_dict={x:x_batch,y:y_batch})
        t_l, t_a=sesh.run([loss,accuracy],feed_dict={x:test_x,y:test_y})
        
        if i>0 and i % 500==0:
            print("Step",i,"of", num_batches,"loss",l,"accuracy",a)
            print("Test loss", t_l,"accuracy",t_a)
        if i > 0 and i % 500==0:
            saver.save(sesh, "logdir\\lstm_model.ckpt")
            writer.flush()
            writer.close()
            
tf.summary.FileWriter("logs", g).close()



