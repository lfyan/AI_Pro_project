{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42693,
     "status": "ok",
     "timestamp": 1552190647309,
     "user": {
      "displayName": "Junling Hu",
      "photoUrl": "https://lh3.googleusercontent.com/-P3fKHkKaoKU/AAAAAAAAAAI/AAAAAAAAAR0/iRPnQiaiVHQ/s64/photo.jpg",
      "userId": "18085096256559291830"
     },
     "user_tz": 480
    },
    "id": "5jNhs2klwkIx",
    "outputId": "f02c216d-59c8-4c6a-88ae-cceac0f405c3"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44880,
     "status": "ok",
     "timestamp": 1552190649656,
     "user": {
      "displayName": "Junling Hu",
      "photoUrl": "https://lh3.googleusercontent.com/-P3fKHkKaoKU/AAAAAAAAAAI/AAAAAAAAAR0/iRPnQiaiVHQ/s64/photo.jpg",
      "userId": "18085096256559291830"
     },
     "user_tz": 480
    },
    "id": "Mvf09fjugFU_",
    "outputId": "bd422aad-1e73-46a1-b685-f5a902dd3825"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in sentence\n",
    "\n",
    "df['clean_text']=df['question_text'].apply(lambda x:[t for t in word_tokenize(x.lower())]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=np.hstack(df.clean_text) # combine list in column clean_text as 1 big list for building embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18866937"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list) # total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263186"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(word_list)) # no of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "## Embedding Vector Creation using Word2Vec Implementation\n",
    "###  Build the dictionary (rare words replaced with UNK token), and give each word an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61176,
     "status": "ok",
     "timestamp": 1552190666150,
     "user": {
      "displayName": "Junling Hu",
      "photoUrl": "https://lh3.googleusercontent.com/-P3fKHkKaoKU/AAAAAAAAAAI/AAAAAAAAAR0/iRPnQiaiVHQ/s64/photo.jpg",
      "userId": "18085096256559291830"
     },
     "user_tz": 480
    },
    "id": "gAL1EECXeZsD",
    "outputId": "3b99d651-f372-4ed1-fd8a-a0e6cfcde78a"
   },
   "outputs": [],
   "source": [
    "vocabularyN = 200000   #The number of unique words we use\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    # count will be an array of (word, frequency), count= [['a', 3], ['this', 4],...]\n",
    "\n",
    "    # wordCount is an array with each with each element with its count  [['a', 3], ['this', 4],...]\n",
    "    wordCount=Counter(words)\n",
    "    print ('length of wordCount',len(wordCount))\n",
    "    commonCount=wordCount.most_common(vocabularyN - 1)  #sort the array by counter, pick the top 49999 words\n",
    "    print ('Most common words', commonCount[:4])\n",
    "  \n",
    "    count.extend(commonCount)  #Add the common words to 'count' array, len(count)=50000\n",
    "\n",
    "    #Create dictionary for the most common words\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) \n",
    "        #we increase the size of dictionary each time, the value is the index\n",
    "    \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:  #go through every word in current data\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)  \n",
    "    #count.append(['UNK',unk_count])\n",
    "  \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of wordCount 263186\n",
      "Most common words [('?', 1381192), ('the', 665368), ('what', 470692), ('is', 446267)]\n"
     ]
    }
   ],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "### Define a function that generate a training data from each batch based on the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "w9APjA-zmfjV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "batchN = 60   #train_x size, the size for training data\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "skip_window = 1 # How many words to consider left and right. in \"2-gram\" skip window=1\n",
    "   #take maximum 3 words to the right and left\n",
    "\n",
    "#Build training data for this batch\n",
    "data_index = 0\n",
    "def generate_training(batchN, num_skips, skip_window):\n",
    "    global data_index  #This allows us to modify global variable inside a function\n",
    "  \n",
    "    assert batchN % num_skips == 0    #The batach size can be evenly divided by number of skips\n",
    "    assert num_skips <= 2 * skip_window  #we don't do too may skips\n",
    "\n",
    "    train_x = np.ndarray(shape=(batchN), dtype=np.int32)  #an array to hold training training data points\n",
    "    labels = np.ndarray(shape=(batchN, 1), dtype=np.int32)\n",
    "\n",
    "    \"\"\"Read data of size span into buffer \"\"\" \n",
    "    ## Span is the total number of words, before + after+ current word\n",
    "    span = 2 * skip_window + 1      \n",
    "    buffer = collections.deque(maxlen=span) #an empty container with size \"span\" (3)\n",
    "\n",
    "    #Get 3 (span) words from the data list\n",
    "    for _ in range(span):  #do it 3 times (when span=3)\n",
    "        buffer.append(data[data_index])\n",
    "        #data_index keeps track of where the current center word (input) is\n",
    "        data_index = data_index + 1 #update data_index\n",
    "        #data_index = (data_index + 1) % len(data)  #u%len(data) is needed only when we have small data  \n",
    "\n",
    "    \"\"\" The actual number of operation is batch_size // num_skips, where \"//\" is floor division, removing decimal numbers \"\"\" \n",
    "    #do it for half of the batch size, as we will do left and right  \n",
    "    for i in range(batchN //num_skips):   # i=0,1,2,3,4 when batchN=10, and num_skips=2\n",
    "        target = skip_window  # initialize the target index to be the center word, so that we can avoid it\n",
    "        targets_to_avoid = [ skip_window ] #once chosen, we won't use it next time\n",
    "\n",
    "        # for every word at target position, we create training data points \n",
    "        for j in range(num_skips):  #j=0,1\n",
    "            while target in targets_to_avoid:  #if target still active, re-sample again\n",
    "                target = random.randint(0, span - 1)  #Return a random integer 0<= N <= 2  (span -1) \n",
    "            #If the sampled index is not in the target_to_avoid  \n",
    "            targets_to_avoid.append(target)  # we don't use target for training data now\n",
    "            #when i=0, j=0, train_x[0] is update with the center word in buffer\n",
    "            train_x[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target] \n",
    "\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return train_x, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ofd1MbBuwiva"
   },
   "source": [
    "###  Create a neural network with 100 nodes on the hidden layer, define the loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61961,
     "status": "ok",
     "timestamp": 1552190667197,
     "user": {
      "displayName": "Junling Hu",
      "photoUrl": "https://lh3.googleusercontent.com/-P3fKHkKaoKU/AAAAAAAAAAI/AAAAAAAAAR0/iRPnQiaiVHQ/s64/photo.jpg",
      "userId": "18085096256559291830"
     },
     "user_tz": 480
    },
    "id": "8pQKsV4Vwlzy",
    "outputId": "1b21bbe9-1c71-4685-e7d3-6faa4ec9d3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "embeddingN = 100 # Dimension of the embedding vector.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "# This creates an array of 16 elements, randomly selected from [0,100)\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 50  # this is an arbituary number\n",
    "\n",
    "\"\"\"Defines a Neural network architecture\"\"\"\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "      # Input data.\n",
    "    train_x = tf.placeholder(tf.int32, shape=[batchN])\n",
    "    train_y = tf.placeholder(tf.int32, shape=[batchN, 1])\n",
    "  \n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # The first layer\n",
    "    layer1_weights = tf.Variable(tf.random_uniform([vocabularyN, embeddingN], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(layer1_weights, train_x)  #hidden layer node called \"embed\"\n",
    "  \n",
    "    # The second layer, which is also the output layer.  \n",
    "    weights = tf.Variable( tf.truncated_normal([vocabularyN, embeddingN], stddev=1.0 / math.sqrt(embeddingN)))\n",
    "    biases = tf.Variable(tf.zeros([vocabularyN]))\n",
    "\n",
    "    # Compute the softmax of the output layer, then the loss, but do it a few samples, this reduces computation\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights, biases, train_y, embed, num_sampled, vocabularyN))\n",
    "\n",
    "    # Optimizer.\n",
    "    #optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all layer1_weights.\n",
    "    # We use the cosine distance, which is the A*B/|A||B|  or A/|A| * B/|B|\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(layer1_weights), 1, keep_dims=True))\n",
    "    normalized_weights = layer1_weights / norm    #the dimension of this is 50,000x 128\n",
    "  \n",
    "    #look up the weights for the validataion data, 16 data points, each data point is position index\n",
    "    # it returns an array of 16 elements, with each element has 128 dimensions\n",
    "    valid_weights = tf.nn.embedding_lookup(normalized_weights, valid_dataset)\n",
    "  \n",
    "    #the dimension of valid_weights is 16x100, and the dimension of the 2nd argument is 100x20000\n",
    "    similarity = tf.matmul(valid_weights, tf.transpose(normalized_weights))\n",
    "\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9jlz35xjh11"
   },
   "source": [
    "###  Starts training by calling each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 136693,
     "status": "ok",
     "timestamp": 1552190741995,
     "user": {
      "displayName": "Junling Hu",
      "photoUrl": "https://lh3.googleusercontent.com/-P3fKHkKaoKU/AAAAAAAAAAI/AAAAAAAAAR0/iRPnQiaiVHQ/s64/photo.jpg",
      "userId": "18085096256559291830"
     },
     "user_tz": 480
    },
    "id": "1bQFGceBxrWW",
    "outputId": "d3354ce0-6d61-45d8-c258-8c30e7c65a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 9.18068408966\n",
      "step is  0\n",
      "Nearest to will: sucessfull, cesar, self-spreading, sene, holiness, pytorch, 132,000, cumbric,\n",
      "Nearest to much: 9th, pasteurizer, viswanathan, intp-as, bhascaryacharya, ^4+, fullback, icelandic,\n",
      "Nearest to think: irresistibly, reregister, platter, litterature, larynx, brfl, fame, mass+velocity,\n",
      "Nearest to want: fashawn, candidature, mobilizer, deos, axworthy, gablé, whitespace, kalani,\n",
      "Nearest to between: amore, mice, dfl, rudyard, abstraxi, aramoana, multi-fare, infundibulum,\n",
      "Nearest to work: trireme, avert, iit-hyderabad, ti-nspire, non-baptized, solidifying, sucicide, harvards,\n",
      "Nearest to n't: micra, prepflash, 0.347, physcially, zoning, india.do, dynasty, internatonal,\n",
      "Nearest to 's: usi-teck, .fundamentals, verniers, neon-20, interx, kambakht, touraments, 32y/o,\n",
      "Nearest to can: meninism, economics​, 3pls, jharia, tongan, icsce, speaker/chairman, liberalisation,\n",
      "Nearest to a: iima, ident, glute/butt, 'lagna, cullen, 0.0308, 6ix, s/02/99,\n",
      "Nearest to one: post-1832, pron, nigel-murray, transversals, broadcasting, flute, deoxy, drugs,\n",
      "Nearest to is: ishare, implosion, huskey, hosue, sharmic, elbowing, sanger, bivectors,\n",
      "Nearest to use: 299792458, adie, gastroc, shelf, extraterrestial, cinemagram, those, copyleft,\n",
      "Nearest to of: quartering, 19th, ansys, popeyes, mlp, soibu, uthappa, common-sense,\n",
      "Nearest to life: a57, home..not, web3.js, 6.87m, beachs, barbies, serotypes, phenergan,\n",
      "Nearest to ,: bruegel, obliviousness, chiller, displease, scanf, 'times, stumbleupon, calls.anyone,\n",
      "Average loss at step 2000 : 5.21734191811\n",
      "step is  2000\n",
      "Average loss at step 4000 : 4.21994798088\n",
      "step is  4000\n",
      "Average loss at step 6000 : 3.94766577971\n",
      "step is  6000\n",
      "Average loss at step 8000 : 3.80960217357\n",
      "step is  8000\n",
      "Average loss at step 10000 : 3.69934070313\n",
      "step is  10000\n",
      "In 10000, step  10000\n",
      "Nearest to will: can, should, sene, 2\\pi\\theta, is, -centrifugal, news/notifications, reparated,\n",
      "Nearest to much: min/km, pasteurizer, 9th, peacetime, tin, 180nm, thylane, bhascaryacharya,\n",
      "Nearest to think: reregister, irresistibly, platter, metthi, exhaled, annihilators, brfl, yaoundé,\n",
      "Nearest to want: religously, fashawn, insult/bully, food/water, mahoney, kalani, drm, candidature,\n",
      "Nearest to between: 'fascist, abstraxi, from, rudyard, in, aramoana, amore, adjusted,\n",
      "Nearest to work: iit-hyderabad, 'electrical, iima, sucicide, drupal, solidifying, avert, random-sounding,\n",
      "Nearest to n't: not, india.do, addendum, to, rayong, physcially, you, women,\n",
      "Nearest to 's: is, and, ,, ?, of, in, with, without,\n",
      "Nearest to can: do, should, will, if, would, i, pass-time, is,\n",
      "Nearest to a: the, my, ?, an, ,, your, or, (,\n",
      "Nearest to one: flute, you, your, gastrulation, happyness, rationalists, microoftalmia, lock,\n",
      "Nearest to is: are, does, was, 's, in, has, indoor, if,\n",
      "Nearest to use: 299792458, when, extrapolated, copyleft, abstinence-only, diljit, tray, swetamber,\n",
      "Nearest to of: in, for, and, from, with, ?, 's, about,\n",
      "Nearest to life: web3.js, 6.87m, beachs, jja, spingarn, college2017, transphobic, serotypes,\n",
      "Nearest to ,: ?, and, ., 's, (, or, in, a,\n",
      "Average loss at step 12000 : 3.64292208564\n",
      "step is  12000\n",
      "Average loss at step 14000 : 3.57093537176\n",
      "step is  14000\n",
      "Average loss at step 16000 : 3.54590576947\n",
      "step is  16000\n",
      "Average loss at step 18000 : 3.49232114017\n",
      "step is  18000\n",
      "Average loss at step 20000 : 3.47222955108\n",
      "step is  20000\n",
      "In 10000, step  20000\n",
      "Nearest to will: can, should, would, if, is, 2\\pi\\theta, pro-india, 5002,\n",
      "Nearest to much: pasteurizer, min/km, 9th, tin, thylane, peacetime, bhascaryacharya, 180nm,\n",
      "Nearest to think: reregister, annihilators, platter, brfl, yaoundé, exhaled, theologians, tiling,\n",
      "Nearest to want: religously, insult/bully, mahoney, only, kalani, food/water, fashawn, read,\n",
      "Nearest to between: in, from, for, 'fascist, rudyard, of, and, adjusted,\n",
      "Nearest to work: iit-hyderabad, iima, 'electrical, submerging, sucicide, ofcotton, regius, pay,\n",
      "Nearest to n't: not, rayong, that, to, baggers, ashanti, physcially, india.do,\n",
      "Nearest to 's: and, is, of, ?, ,, without, in, (,\n",
      "Nearest to can: should, will, do, would, if, could, victors, pass-time,\n",
      "Nearest to a: the, your, my, his, ?, any, and, their,\n",
      "Nearest to one: flute, guys, lock, happyness, this, gastrulation, us, someone,\n",
      "Nearest to is: does, are, was, has, 's, would, indoor, in,\n",
      "Nearest to use: 299792458, copyleft, extrapolated, bacherlors, abstinence-only, muliple, when, crisil,\n",
      "Nearest to of: in, for, and, from, about, at, on, after,\n",
      "Nearest to life: web3.js, college2017, 6.87m, bhd, jja, transphobic, copyright/ipr, spingarn,\n",
      "Nearest to ,: ?, ., and, (, or, in, ccb, cannoli,\n",
      "Average loss at step 22000 : 3.42955618632\n",
      "step is  22000\n",
      "Average loss at step 24000 : 3.39737471735\n",
      "step is  24000\n",
      "Average loss at step 26000 : 3.39730428839\n",
      "step is  26000\n",
      "Average loss at step 28000 : 3.37314838767\n",
      "step is  28000\n",
      "Average loss at step 30000 : 3.33423451781\n",
      "step is  30000\n",
      "In 10000, step  30000\n",
      "Nearest to will: can, should, would, could, pro-india, does, did, is,\n",
      "Nearest to much: pasteurizer, min/km, thylane, 9th, 180nm, bhascaryacharya, tin, infedility,\n",
      "Nearest to think: reregister, brfl, annihilators, tiling, yaoundé, 19:23-30, know, evernote,\n",
      "Nearest to want: religously, insult/bully, read, mahoney, kalani, leave, food/water, only,\n",
      "Nearest to between: from, for, in, 'fascist, rudyard, of, adjusted, with,\n",
      "Nearest to work: pay, iit-hyderabad, iima, ofcotton, 'electrical, random-sounding, submerging, drupal,\n",
      "Nearest to n't: not, rayong, that, to, still, really, why, ashanti,\n",
      "Nearest to 's: is, and, was, of, 800b0100, or, ?, (,\n",
      "Nearest to can: should, will, could, would, do, if, victors, did,\n",
      "Nearest to a: my, the, his, any, an, our, your, or,\n",
      "Nearest to one: happyness, guys, us, fans, balidan, flute, lock, negative,\n",
      "Nearest to is: does, was, are, has, 's, did, will, indoor,\n",
      "Nearest to use: 299792458, copyleft, bacherlors, 5x²y/z⁴, extrapolated, autocar, muliple, ₹1.06/minute,\n",
      "Nearest to of: in, and, for, from, on, about, 's, that,\n",
      "Nearest to life: web3.js, copyright/ipr, bhd, college2017, 3017, kyaw, transphobic, 6.87m,\n",
      "Nearest to ,: ?, ., (, and, or, ccb, cannoli, countrys,\n",
      "Average loss at step 32000 : 3.32727273178\n",
      "step is  32000\n",
      "Average loss at step 34000 : 3.30160408533\n",
      "step is  34000\n",
      "Average loss at step 36000 : 3.28948556995\n",
      "step is  36000\n",
      "Average loss at step 38000 : 3.30266620898\n",
      "step is  38000\n",
      "Average loss at step 40000 : 3.26142635345\n",
      "step is  40000\n",
      "In 10000, step  40000\n",
      "Nearest to will: should, would, can, could, pro-india, does, if, did,\n",
      "Nearest to much: pasteurizer, min/km, thylane, many, 180nm, 9th, tin, infedility,\n",
      "Nearest to think: know, annihilators, yaoundé, 19:23-30, reregister, mauryan, brfl, tiling,\n",
      "Nearest to want: religously, insult/bully, read, leave, got, food/water, mahoney, hav,\n",
      "Nearest to between: in, from, during, 'fascist, for, indebt, of, with,\n",
      "Nearest to work: pay, ofcotton, iit-hyderabad, iima, 'electrical, random-sounding, submerging, silencers,\n",
      "Nearest to n't: not, really, still, that, rayong, why, ashanti, baggers,\n",
      "Nearest to 's: is, was, and, of, or, 800b0100, gervais, weatherly,\n",
      "Nearest to can: should, will, could, would, do, if, victors, does,\n",
      "Nearest to a: any, the, his, my, your, or, our, another,\n",
      "Nearest to one: guys, someone, happyness, gastrulation, fans, crt, faceboog, balidan,\n",
      "Nearest to is: does, was, are, has, 's, did, tapered, if,\n",
      "Nearest to use: 299792458, bacherlors, fuan, 5x²y/z⁴, nadagam, autocar, copyleft, extrapolated,\n",
      "Nearest to of: in, for, from, on, about, and, between, 's,\n",
      "Nearest to life: copyright/ipr, college2017, kyaw, bhd, games/projects, transphobic, tpus, 3017,\n",
      "Nearest to ,: ?, ., (, and, or, in, encroaching, countrys,\n",
      "Average loss at step 42000 : 3.2565550375\n",
      "step is  42000\n",
      "Average loss at step 44000 : 3.25746645772\n",
      "step is  44000\n",
      "Average loss at step 46000 : 3.23050900537\n",
      "step is  46000\n",
      "Average loss at step 48000 : 3.23335795909\n",
      "step is  48000\n",
      "before getting final embeddings\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"Initialized\")\n",
    "    average_loss = 0\n",
    "  \n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_training(batchN, num_skips, skip_window)\n",
    "        feed_dict = {train_x : batch_data, train_y : batch_labels}\n",
    "                 \n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                 \n",
    "        average_loss += l\n",
    "    \n",
    "        #The following is only for inspection \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print (\"Average loss at step\", step, \":\", average_loss)\n",
    "            average_loss = 0\n",
    "            print ('step is ',step)\n",
    "      \n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()  #the eval() re-run the variable \"similarity\" and updates its value\n",
    "            if step > 0:\n",
    "                print ('In 10000, step ',step)\n",
    "        \n",
    "            for i in range(valid_size):\n",
    "                #print valid_examples[i]\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = \"Nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = \"%s %s,\" % (log, close_word)\n",
    "                print (log)\n",
    "    \n",
    "    print ('before getting final embeddings')\n",
    "    final_embeddings = normalized_weights.eval()\n",
    "    print ('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 146984,
     "status": "ok",
     "timestamp": 1552190752400,
     "user": {
      "displayName": "Junling Hu",
      "photoUrl": "https://lh3.googleusercontent.com/-P3fKHkKaoKU/AAAAAAAAAAI/AAAAAAAAAR0/iRPnQiaiVHQ/s64/photo.jpg",
      "userId": "18085096256559291830"
     },
     "user_tz": 480
    },
    "id": "aiqEJTaIWQ74",
    "outputId": "3ea575c5-d57f-4cb3-9d5e-630a1b724460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "(200000, 100)\n"
     ]
    }
   ],
   "source": [
    "print (len(final_embeddings))\n",
    "print(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save final_embedding ,dictionary, reverse_dictionary\n",
    "np.save('dictionary.npy', dictionary) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt('final_embeddings.txt', final_embeddings, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordindex(x):\n",
    "    if x in dictionary:\n",
    "        return dictionary[x]\n",
    "    else:\n",
    "        return dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure words are all in dictionary\n",
    "\n",
    "df['clean_text_idx']=df['clean_text'].apply(lambda x:[wordindex(t) for t in x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum sentence size\n",
    "\n",
    "max(df.clean_text.apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1306122"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1225312\n",
       "1    1000000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsamling minority class\n",
    "\n",
    "df_majority = df[df.target==0]\n",
    "df_minority = df[df.target==1]\n",
    " \n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=1000000,    # to match majority class\n",
    "                                 random_state=101) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding data to a fix size\n",
    "\n",
    "data_pad=pad_sequences(df_upsampled['clean_text_idx'],maxlen=412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225312, 412)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df_upsampled['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=to_categorical(labels,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('labels_word2vec_method.txt', labels, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data_pad_word2vec_method.txt', data_pad, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden_units=128  # no of hidden units in 1 lstm cell\n",
    "num_classes=2\n",
    "embedding_size=100  # embedding vector size\n",
    "sentence_len=412  # max sentence length in dataset\n",
    "vocab_size=200000  # Glove vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    x=tf.placeholder(tf.int32,shape=[None,sentence_len])  # input place holder\n",
    "    y=tf.placeholder(tf.float32,shape=[None,2]) # output place holder\n",
    "    \n",
    "    w=tf.Variable(tf.random_normal([lstm_hidden_units,num_classes]))\n",
    "    b=tf.Variable(tf.constant(0.1,shape=[num_classes]))\n",
    "        \n",
    "    Embedding = tf.get_variable(name=\"word_embedding\", shape=[final_embeddings.shape[0],embedding_size],\n",
    "                                                       initializer=tf.constant_initializer(final_embeddings),\n",
    "                                                       trainable=False) # loading embedding matrix                               \n",
    "    embed_lookup=tf.nn.embedding_lookup(Embedding,x) # batch_size x sentence_length x embedding_size\n",
    "    \n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(lstm_hidden_units) # create LSTM layer\n",
    "    current_batch_size=tf.shape(x)[0]\n",
    "    initial_state=lstm_cell.zero_state(current_batch_size,dtype=tf.float32) # state initialisations\n",
    "\n",
    "    outputs, _ =tf.nn.dynamic_rnn(lstm_cell,embed_lookup,initial_state=initial_state,dtype=tf.float32)#batch_size x sen_length x hidden_units \n",
    "    outputs=tf.transpose(outputs,[1,0,2]) #sentence_length x batch_size x hidden_units\n",
    "    last=tf.gather(outputs,int(outputs.get_shape()[0])-1)  # # batch_size x hidden_units\n",
    "    \n",
    "    predictions=tf.matmul(last,w)+b # batch_size x 2\n",
    "    correct_predictions=tf.equal(tf.argmax(tf.nn.sigmoid(predictions),axis=1),tf.argmax(y,axis=1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_predictions,tf.float32))     \n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions,labels=y))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.005).minimize(loss)\n",
    "    \n",
    "    return optimizer,loss,x,y,accuracy,predictions, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train,y_test=train_test_split(data_pad,labels,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "optimizer,loss,x,y,accuracy,predictions, correct_predictions=model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_batches=len(X_train)//batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 of 48678 loss 0.432214 accuracy 0.78125\n",
      "Test loss 0.398796 accuracy 0.8\n",
      "Step 1000 of 48678 loss 0.403539 accuracy 0.8125\n",
      "Test loss 0.385915 accuracy 0.825\n",
      "Step 1500 of 48678 loss 0.352263 accuracy 0.8125\n",
      "Test loss 0.468525 accuracy 0.795\n",
      "Step 2000 of 48678 loss 0.61345 accuracy 0.6875\n",
      "Test loss 0.350355 accuracy 0.87\n",
      "Step 2500 of 48678 loss 0.223349 accuracy 0.9375\n",
      "Test loss 0.424124 accuracy 0.81\n",
      "Step 3000 of 48678 loss 0.236677 accuracy 0.9375\n",
      "Test loss 0.381306 accuracy 0.82\n",
      "Step 3500 of 48678 loss 0.290952 accuracy 0.90625\n",
      "Test loss 0.434748 accuracy 0.815\n",
      "Step 4000 of 48678 loss 0.352875 accuracy 0.84375\n",
      "Test loss 0.388232 accuracy 0.84\n",
      "Step 4500 of 48678 loss 0.266288 accuracy 0.84375\n",
      "Test loss 0.353286 accuracy 0.855\n",
      "Step 5000 of 48678 loss 0.34254 accuracy 0.84375\n",
      "Test loss 0.320817 accuracy 0.855\n",
      "Step 5500 of 48678 loss 0.382165 accuracy 0.8125\n",
      "Test loss 0.313459 accuracy 0.885\n",
      "Step 6000 of 48678 loss 0.273566 accuracy 0.9375\n",
      "Test loss 0.279896 accuracy 0.895\n",
      "Step 6500 of 48678 loss 0.454796 accuracy 0.6875\n",
      "Test loss 0.325805 accuracy 0.87\n",
      "Step 7000 of 48678 loss 0.348845 accuracy 0.8125\n",
      "Test loss 0.396096 accuracy 0.845\n",
      "Step 7500 of 48678 loss 0.440336 accuracy 0.78125\n",
      "Test loss 0.300018 accuracy 0.87\n",
      "Step 8000 of 48678 loss 0.353563 accuracy 0.84375\n",
      "Test loss 0.35057 accuracy 0.885\n",
      "Step 8500 of 48678 loss 0.228989 accuracy 0.875\n",
      "Test loss 0.27846 accuracy 0.895\n",
      "Step 9000 of 48678 loss 0.395972 accuracy 0.875\n",
      "Test loss 0.247307 accuracy 0.905\n",
      "Step 9500 of 48678 loss 0.219191 accuracy 0.90625\n",
      "Test loss 0.278045 accuracy 0.875\n",
      "Step 10000 of 48678 loss 0.348763 accuracy 0.8125\n",
      "Test loss 0.305694 accuracy 0.87\n",
      "Step 10500 of 48678 loss 0.166727 accuracy 0.90625\n",
      "Test loss 0.283819 accuracy 0.85\n",
      "Step 11000 of 48678 loss 0.374018 accuracy 0.84375\n",
      "Test loss 0.350784 accuracy 0.84\n",
      "Step 11500 of 48678 loss 0.291812 accuracy 0.875\n",
      "Test loss 0.276679 accuracy 0.9\n",
      "Step 12000 of 48678 loss 0.417431 accuracy 0.8125\n",
      "Test loss 0.32991 accuracy 0.865\n",
      "Step 12500 of 48678 loss 0.171148 accuracy 0.9375\n",
      "Test loss 0.331879 accuracy 0.865\n",
      "Step 13000 of 48678 loss 0.178308 accuracy 0.9375\n",
      "Test loss 0.277768 accuracy 0.88\n",
      "Step 13500 of 48678 loss 0.237087 accuracy 0.9375\n",
      "Test loss 0.309372 accuracy 0.875\n",
      "Step 14000 of 48678 loss 0.186808 accuracy 0.9375\n",
      "Test loss 0.290478 accuracy 0.905\n",
      "Step 14500 of 48678 loss 0.4895 accuracy 0.78125\n",
      "Test loss 0.242515 accuracy 0.92\n",
      "Step 15000 of 48678 loss 0.331219 accuracy 0.84375\n",
      "Test loss 0.344817 accuracy 0.855\n",
      "Step 15500 of 48678 loss 0.261339 accuracy 0.96875\n",
      "Test loss 0.247946 accuracy 0.9\n",
      "Step 16000 of 48678 loss 0.439261 accuracy 0.84375\n",
      "Test loss 0.259074 accuracy 0.91\n",
      "Step 16500 of 48678 loss 0.226756 accuracy 0.90625\n",
      "Test loss 0.333802 accuracy 0.86\n",
      "Step 17000 of 48678 loss 0.4899 accuracy 0.78125\n",
      "Test loss 0.302523 accuracy 0.905\n",
      "Step 17500 of 48678 loss 0.411104 accuracy 0.8125\n",
      "Test loss 0.285789 accuracy 0.9\n",
      "Step 18000 of 48678 loss 0.26049 accuracy 0.9375\n",
      "Test loss 0.284975 accuracy 0.875\n",
      "Step 18500 of 48678 loss 0.216529 accuracy 0.90625\n",
      "Test loss 0.218401 accuracy 0.935\n",
      "Step 19000 of 48678 loss 0.252079 accuracy 0.84375\n",
      "Test loss 0.210301 accuracy 0.92\n",
      "Step 19500 of 48678 loss 0.278368 accuracy 0.9375\n",
      "Test loss 0.385006 accuracy 0.825\n",
      "Step 20000 of 48678 loss 0.273544 accuracy 0.90625\n",
      "Test loss 0.408038 accuracy 0.795\n",
      "Step 20500 of 48678 loss 0.445696 accuracy 0.8125\n",
      "Test loss 0.321306 accuracy 0.84\n",
      "Step 21000 of 48678 loss 0.344155 accuracy 0.84375\n",
      "Test loss 0.459992 accuracy 0.78\n",
      "Step 21500 of 48678 loss 0.482677 accuracy 0.78125\n",
      "Test loss 0.460186 accuracy 0.785\n",
      "Step 22000 of 48678 loss 0.432764 accuracy 0.8125\n",
      "Test loss 0.417129 accuracy 0.825\n",
      "Step 22500 of 48678 loss 0.297257 accuracy 0.84375\n",
      "Test loss 0.501946 accuracy 0.785\n",
      "Step 23000 of 48678 loss 0.275621 accuracy 0.84375\n",
      "Test loss 0.426143 accuracy 0.805\n",
      "Step 23500 of 48678 loss 0.260832 accuracy 0.84375\n",
      "Test loss 0.459633 accuracy 0.8\n",
      "Step 24000 of 48678 loss 0.328753 accuracy 0.84375\n",
      "Test loss 0.392259 accuracy 0.84\n",
      "Step 24500 of 48678 loss 0.572012 accuracy 0.75\n",
      "Test loss 0.33011 accuracy 0.87\n",
      "Step 25000 of 48678 loss 0.184417 accuracy 0.9375\n",
      "Test loss 0.359459 accuracy 0.845\n",
      "Step 25500 of 48678 loss 0.414973 accuracy 0.8125\n",
      "Test loss 0.242862 accuracy 0.9\n",
      "Step 26000 of 48678 loss 0.399617 accuracy 0.75\n",
      "Test loss 0.348442 accuracy 0.845\n",
      "Step 26500 of 48678 loss 0.167288 accuracy 0.96875\n",
      "Test loss 0.26902 accuracy 0.91\n",
      "Step 27000 of 48678 loss 0.326474 accuracy 0.90625\n",
      "Test loss 0.320343 accuracy 0.895\n",
      "Step 27500 of 48678 loss 0.233038 accuracy 0.875\n",
      "Test loss 0.300998 accuracy 0.88\n",
      "Step 28000 of 48678 loss 0.351294 accuracy 0.78125\n",
      "Test loss 0.339273 accuracy 0.865\n",
      "Step 28500 of 48678 loss 0.249251 accuracy 0.90625\n",
      "Test loss 0.357051 accuracy 0.84\n",
      "Step 29000 of 48678 loss 0.247299 accuracy 0.90625\n",
      "Test loss 0.245522 accuracy 0.905\n",
      "Step 29500 of 48678 loss 0.405695 accuracy 0.84375\n",
      "Test loss 0.303275 accuracy 0.88\n",
      "Step 30000 of 48678 loss 0.22058 accuracy 0.90625\n",
      "Test loss 0.267504 accuracy 0.895\n",
      "Step 30500 of 48678 loss 0.492259 accuracy 0.8125\n",
      "Test loss 0.273267 accuracy 0.915\n",
      "Step 31000 of 48678 loss 0.146552 accuracy 0.9375\n",
      "Test loss 0.240135 accuracy 0.91\n",
      "Step 31500 of 48678 loss 0.392801 accuracy 0.84375\n",
      "Test loss 0.268565 accuracy 0.885\n",
      "Step 32000 of 48678 loss 0.0972098 accuracy 1.0\n",
      "Test loss 0.311586 accuracy 0.87\n",
      "Step 32500 of 48678 loss 0.366665 accuracy 0.84375\n",
      "Test loss 0.232612 accuracy 0.895\n",
      "Step 33000 of 48678 loss 0.163473 accuracy 0.96875\n",
      "Test loss 0.194364 accuracy 0.925\n",
      "Step 33500 of 48678 loss 0.0975812 accuracy 0.96875\n",
      "Test loss 0.196274 accuracy 0.92\n",
      "Step 34000 of 48678 loss 0.377488 accuracy 0.8125\n",
      "Test loss 0.243461 accuracy 0.91\n",
      "Step 34500 of 48678 loss 0.159011 accuracy 0.96875\n",
      "Test loss 0.249552 accuracy 0.915\n",
      "Step 35000 of 48678 loss 0.254463 accuracy 0.875\n",
      "Test loss 0.261554 accuracy 0.875\n",
      "Step 35500 of 48678 loss 0.13854 accuracy 0.96875\n",
      "Test loss 0.268164 accuracy 0.875\n",
      "Step 36000 of 48678 loss 0.333635 accuracy 0.875\n",
      "Test loss 0.293672 accuracy 0.905\n",
      "Step 36500 of 48678 loss 0.285401 accuracy 0.84375\n",
      "Test loss 0.253738 accuracy 0.88\n",
      "Step 37000 of 48678 loss 0.285229 accuracy 0.875\n",
      "Test loss 0.274322 accuracy 0.89\n",
      "Step 37500 of 48678 loss 0.437574 accuracy 0.90625\n",
      "Test loss 0.285725 accuracy 0.88\n",
      "Step 38000 of 48678 loss 0.21164 accuracy 0.9375\n",
      "Test loss 0.24797 accuracy 0.895\n",
      "Step 38500 of 48678 loss 0.131352 accuracy 0.96875\n",
      "Test loss 0.263461 accuracy 0.895\n",
      "Step 39000 of 48678 loss 0.193623 accuracy 0.90625\n",
      "Test loss 0.2602 accuracy 0.885\n",
      "Step 39500 of 48678 loss 0.300043 accuracy 0.84375\n",
      "Test loss 0.198337 accuracy 0.905\n",
      "Step 40000 of 48678 loss 0.415509 accuracy 0.71875\n",
      "Test loss 0.270568 accuracy 0.895\n",
      "Step 40500 of 48678 loss 0.448061 accuracy 0.8125\n",
      "Test loss 0.25496 accuracy 0.885\n",
      "Step 41000 of 48678 loss 0.123731 accuracy 0.96875\n",
      "Test loss 0.251276 accuracy 0.9\n",
      "Step 41500 of 48678 loss 0.25568 accuracy 0.90625\n",
      "Test loss 0.312521 accuracy 0.87\n",
      "Step 42000 of 48678 loss 0.247763 accuracy 0.875\n",
      "Test loss 0.307196 accuracy 0.875\n",
      "Step 42500 of 48678 loss 0.221876 accuracy 0.875\n",
      "Test loss 0.199648 accuracy 0.905\n",
      "Step 43000 of 48678 loss 0.290365 accuracy 0.875\n",
      "Test loss 0.36272 accuracy 0.855\n",
      "Step 43500 of 48678 loss 0.212559 accuracy 0.9375\n",
      "Test loss 0.22615 accuracy 0.92\n",
      "Step 44000 of 48678 loss 0.134415 accuracy 0.96875\n",
      "Test loss 0.288968 accuracy 0.885\n",
      "Step 44500 of 48678 loss 0.323925 accuracy 0.8125\n",
      "Test loss 0.315096 accuracy 0.865\n",
      "Step 45000 of 48678 loss 0.23764 accuracy 0.9375\n",
      "Test loss 0.325767 accuracy 0.855\n",
      "Step 45500 of 48678 loss 0.189542 accuracy 0.9375\n",
      "Test loss 0.270604 accuracy 0.88\n",
      "Step 46000 of 48678 loss 0.171971 accuracy 0.96875\n",
      "Test loss 0.241404 accuracy 0.92\n",
      "Step 46500 of 48678 loss 0.274394 accuracy 0.875\n",
      "Test loss 0.268159 accuracy 0.9\n",
      "Step 47000 of 48678 loss 0.190877 accuracy 0.9375\n",
      "Test loss 0.227281 accuracy 0.92\n",
      "Step 47500 of 48678 loss 0.239116 accuracy 0.90625\n",
      "Test loss 0.289845 accuracy 0.885\n",
      "Step 48000 of 48678 loss 0.151995 accuracy 0.9375\n",
      "Test loss 0.290335 accuracy 0.895\n",
      "Step 48500 of 48678 loss 0.243237 accuracy 0.875\n",
      "Test loss 0.27015 accuracy 0.89\n",
      "Average accuracy 0.87299\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sesh:\n",
    "    init=tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sesh.run(init)    \n",
    "    saver = tf.train.Saver()\n",
    "    writer= tf.summary.FileWriter(\"logdir/\", graph = sesh.graph)    \n",
    "    test_accuracy=[]\n",
    "    test_p=[]\n",
    "    test_r=[]\n",
    "    for i in range(num_batches):\n",
    "        if i !=num_batches-1:\n",
    "            x_batch=X_train[i*batch_size:i*batch_size+batch_size]\n",
    "            y_batch=y_train[i*batch_size:i*batch_size+batch_size]\n",
    "        else:\n",
    "            x_batch=X_train[i*batch_size:]\n",
    "            y_batch=y_train[i*batch_size:]        \n",
    "        \n",
    "        _, l, a=sesh.run([optimizer,loss,accuracy],feed_dict={x:x_batch,y:y_batch})\n",
    "               \n",
    "        if i>0 and i % 500==0:\n",
    "             # Randomly setting testing data to see performance when training\n",
    "            rand_idx = np.random.choice(np.arange(len(X_test)),200, replace=False)\n",
    "            test_x = X_test[rand_idx]\n",
    "            test_y = y_test[rand_idx]\n",
    "            t_l, t_a=sesh.run([loss,accuracy],feed_dict={x:test_x,y:test_y})\n",
    "            test_accuracy.append(t_a)           \n",
    "            print(\"Step\",i,\"of\", num_batches,\"loss\",l,\"accuracy\",a)\n",
    "            print(\"Test loss\", t_l,\"accuracy\",t_a,)            \n",
    "        \n",
    "    print(\"Average accuracy\", np.mean(test_accuracy)) \n",
    "    saver.save(sesh, \"logdir\\\\lstm_model.ckpt\")\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec_JL_1.ipynb",
   "provenance": [
    {
     "file_id": "1dVKtT2wlh3o8tFBtpLRRpWCThRJlNVWi",
     "timestamp": 1552192328252
    },
    {
     "file_id": "1JoLKThmVppR7zbe4TulGNYsJ-SbSiUL0",
     "timestamp": 1551989001009
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
